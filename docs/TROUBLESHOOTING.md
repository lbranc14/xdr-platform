# üîß Guide de Troubleshooting - XDR Platform

Ce document d√©taille les probl√©matiques rencontr√©es durant le d√©veloppement et leurs solutions.

---

## 1. Gestion des Arrays PostgreSQL (Tags)

### üî¥ Probl√®me

```
sql: Scan error on column index 11, name "tags": unsupported Scan, 
storing driver.Value type []uint8 into type *[]string
```

L'API Gateway retournait une erreur lors de la lecture des √©v√©nements. Les tags √©taient retourn√©s vides `[]` au lieu de contenir les valeurs.

### üîç Analyse

PostgreSQL stocke les arrays avec un format sp√©cifique `{val1,val2,val3}`. Le driver Go `lib/pq` n√©cessite l'utilisation de `pq.Array()` pour convertir correctement entre les types PostgreSQL et Go.

Le code utilisait :
```go
var tagsRaw interface{}
err := rows.Scan(..., &tagsRaw, ...)
event.Tags = []string{} // Workaround temporaire
```

### ‚úÖ Solution

Remplacement dans 3 fonctions (`GetRecentEvents`, `GetFilteredEvents`, `scanEvents`) :

```go
// Import requis
import "github.com/lib/pq"

// Avant (incorrect)
var tagsRaw interface{}
err := rows.Scan(..., &tagsRaw, ...)

// Apr√®s (correct)
err := rows.Scan(..., pq.Array(&event.Tags), ...)
```

**Fichiers modifi√©s** :
- `api-gateway/database/timescale.go` (lignes ~177, ~416)

**Commit** :
```bash
git add api-gateway/database/timescale.go
git commit -m "fix: use pq.Array() for PostgreSQL TEXT[] tags scanning"
```

---

## 2. ImageInspectError sur Tous les Pods Kubernetes

### üî¥ Probl√®me

```
ImageInspectError: short name mode is enforcing, but image name redis:7-alpine 
returns ambiguous list
```

Tous les pods (infrastructure + applicatifs) restaient bloqu√©s en `ImagePullBackOff`. Aucun service ne pouvait d√©marrer.

### üîç Analyse

Kubernetes sur Oracle Cloud n√©cessite des **noms d'images complets** avec le registry explicite. Les noms courts (`redis:7-alpine`) sont ambigus car ils peuvent pointer vers plusieurs registries :
- docker.io/redis:7-alpine (Docker Hub)
- quay.io/redis:7-alpine (Quay.io)
- etc.

Le cluster est configur√© en "short name mode enforcing" pour des raisons de s√©curit√©.

### ‚úÖ Solution

Pr√©fixage de **TOUTES** les images avec `docker.io/` :

**Infrastructure** :
- `redis:7-alpine` ‚Üí `docker.io/redis:7-alpine`
- `timescale/timescaledb:latest-pg15` ‚Üí `docker.io/timescale/timescaledb:latest-pg15`
- `confluentinc/cp-kafka:7.5.0` ‚Üí `docker.io/confluentinc/cp-kafka:7.5.0`
- `confluentinc/cp-zookeeper:7.5.0` ‚Üí `docker.io/confluentinc/cp-zookeeper:7.5.0`

**Applications** :
- `lbranc14/xdr-agent:latest` ‚Üí `docker.io/lbranc14/xdr-agent:latest`
- `lbranc14/xdr-ingestion:latest` ‚Üí `docker.io/lbranc14/xdr-ingestion:latest`
- `lbranc14/xdr-api-gateway:latest` ‚Üí `docker.io/lbranc14/xdr-api-gateway:latest`
- `lbranc14/xdr-frontend:latest` ‚Üí `docker.io/lbranc14/xdr-frontend:latest`

**Fichiers modifi√©s** : 13 manifests YAML (tous les deployments et jobs)

**Commit** :
```bash
git add kubernetes/*.yaml
git commit -m "fix(k8s): add docker.io prefix to all images for Oracle Cloud"
```

---

## 3. TimescaleDB CrashLoopBackOff (lost+found)

### üî¥ Probl√®me

```
initdb: error: directory "/var/lib/postgresql/data" exists but is not empty
initdb: detail: It contains a lost+found directory, perhaps due to it being a mount point.
initdb: hint: Using a mount point directly as the data directory is not recommended.
```

TimescaleDB entrait en `CrashLoopBackOff`. La base de donn√©es ne pouvait pas d√©marrer.

### üîç Analyse

Les volumes persistants (Block Storage) sur Oracle Cloud sont format√©s avec un syst√®me de fichiers qui cr√©e automatiquement un dossier `lost+found` √† la racine. 

PostgreSQL refuse de s'initialiser dans un dossier non vide. Le volume √©tait mont√© directement sur `/var/lib/postgresql/data`, ce qui n'est pas recommand√©.

### ‚úÖ Solution

Ajout d'un `subPath` dans le manifest Kubernetes :

```yaml
# Avant
volumeMounts:
- name: timescaledb-storage
  mountPath: /var/lib/postgresql/data

# Apr√®s
volumeMounts:
- name: timescaledb-storage
  mountPath: /var/lib/postgresql/data
  subPath: pgdata  # ‚Üê Ajout

env:
- name: PGDATA
  value: /var/lib/postgresql/data/pgdata  # ‚Üê Ajout
```

**Actions suppl√©mentaires** :
1. Suppression du namespace complet pour nettoyer les PVCs
2. Red√©ploiement avec la nouvelle configuration

**Fichiers modifi√©s** :
- `kubernetes/10-timescaledb.yaml`

**Commit** :
```bash
git add kubernetes/10-timescaledb.yaml
git commit -m "fix(k8s): add subPath for TimescaleDB volume to avoid lost+found conflict"
```

---

## 4. Frontend CrashLoopBackOff (R√©solution DNS)

### üî¥ Probl√®me

```
nginx: [emerg] host not found in upstream "api-gateway" in 
/etc/nginx/conf.d/default.conf:26
```

Le frontend NGINX crashait au d√©marrage. Dashboard inaccessible.

### üîç Analyse

Le fichier `nginx.conf` du frontend contenait :
```nginx
proxy_pass http://api-gateway:8000;
```

Mais le service Kubernetes √©tait nomm√© `api-gateway-service`. NGINX ne pouvait pas r√©soudre le nom DNS `api-gateway` dans le cluster.

### ‚úÖ Solution

**Option choisie** : Renommer le service Kubernetes pour correspondre au nom dans nginx.conf

```yaml
# Dans 22-api-gateway.yaml
apiVersion: v1
kind: Service
metadata:
  name: api-gateway  # Chang√© de "api-gateway-service"
  namespace: xdr-platform
```

**Alternative non retenue** : Modifier nginx.conf et rebuild l'image Docker (plus long)

**Fichiers modifi√©s** :
- `kubernetes/22-api-gateway.yaml`

**Commit** :
```bash
git add kubernetes/22-api-gateway.yaml
git commit -m "fix(k8s): rename api-gateway service for nginx DNS resolution"
```

---

## 5. Erreur "failed to unmarshal raw_data"

### üî¥ Probl√®me

```json
{
  "details": "failed to unmarshal raw_data: invalid character 'E' looking for beginning of value",
  "error": "Failed to retrieve events"
}
```

L'API retournait une erreur HTTP 500. Dashboard affichait "Failed to fetch events".

### üîç Analyse

Le script SQL de g√©n√©ration de donn√©es de test ins√©rait `raw_data` comme du **texte brut** :
```sql
raw_data := 'Event 1: system on web-server-01'
```

Mais l'API Gateway s'attendait √† du **JSON** et tentait de parser avec `json.Unmarshal()`.

### ‚úÖ Solution

R√©√©criture du script SQL pour g√©n√©rer du JSON valide :

```sql
raw_data_json := jsonb_build_object(
    'event_id', i,
    'message', 'Event ' || i || ': ' || rand_event_type || ' on ' || rand_hostname,
    'details', jsonb_build_object(
        'action', rand_event_type,
        'target', rand_hostname,
        'user', rand_username
    )
);

INSERT INTO raw_events (..., raw_data) VALUES (..., raw_data_json);
```

**Actions** :
1. `TRUNCATE TABLE raw_events;` pour vider la table
2. R√©ex√©cution du script SQL corrig√©
3. V√©rification : 500 √©v√©nements ins√©r√©s avec JSON valide

**Fichiers modifi√©s** :
- `docs/schema.sql` (script de g√©n√©ration de donn√©es)

---

## 6. Let's Encrypt Timeout (Firewall)

### üî¥ Probl√®me

```
Failed to register ACME account: Get "https://acme-v02.api.letsencrypt.org/directory": 
dial tcp 172.65.32.248:443: i/o timeout
```

Le ClusterIssuer cert-manager restait bloqu√© en `READY=False`. Impossible d'obtenir un certificat SSL valide.

### üîç Analyse

Le cluster Kubernetes sur Oracle Cloud a des **r√®gles de s√©curit√© r√©seau (Security Lists)** qui bloquent les connexions sortantes HTTPS vers Internet par d√©faut.

cert-manager ne peut pas contacter les serveurs ACME de Let's Encrypt pour valider le domaine et obtenir un certificat.

### ‚úÖ Solution

**Solution choisie** : Certificat auto-sign√©

Cr√©ation d'un ClusterIssuer et Certificate **auto-sign√©s** :

```yaml
# selfsigned-cert.yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: selfsigned-issuer
spec:
  selfSigned: {}
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: xdr-tls-selfsigned
  namespace: xdr-platform
spec:
  secretName: xdr-tls-secret
  duration: 2160h # 90 jours
  dnsNames:
    - xdr-platform.duckdns.org
  issuerRef:
    name: selfsigned-issuer
    kind: ClusterIssuer
```

**Modifications Ingress** :
Suppression de l'annotation qui causait la cr√©ation automatique de certificat Let's Encrypt :
```yaml
# Comment√©/supprim√© :
# cert-manager.io/cluster-issuer: "letsencrypt-prod"
```

**Ordre de d√©ploiement critique** :
1. Cr√©er le certificat auto-sign√©
2. Attendre `kubectl wait --for=condition=Ready certificate/...`
3. D√©ployer l'Ingress

**R√©sultat** :
- HTTPS fonctionnel avec certificat auto-sign√©
- Avertissement navigateur attendu ("Non s√©curis√©") mais connexion chiffr√©e
- Acceptable pour environnement de test/portfolio

**Note pour production** :
En environnement professionnel, il faudrait ouvrir les r√®gles de s√©curit√© r√©seau ou utiliser un reverse proxy externe.

**Fichiers modifi√©s** :
- `kubernetes/selfsigned-cert.yaml` (nouveau)
- `kubernetes/ingress-https.yaml` (annotation supprim√©e)

**Commit** :
```bash
git add kubernetes/selfsigned-cert.yaml kubernetes/ingress-https.yaml
git commit -m "fix(k8s): use self-signed certificate due to Oracle Cloud firewall"
```

---

## 7. Manque de RAM (2 GB Total)

### üî¥ Probl√®me

Instances `VM.Standard.E3.Flex` du Free Tier ont seulement **1 GB RAM par node** (2 GB total). Kafka et Zookeeper entraient r√©guli√®rement en `CrashLoopBackOff`.

### üîç Analyse

Les limites de ressources dans les manifests Kubernetes √©taient trop √©lev√©es pour le mat√©riel disponible. Par exemple, TimescaleDB demandait initialement 1 GB RAM = 50% d'un node.

### ‚úÖ Solution

**1. R√©duction drastique des ressources** :

```yaml
resources:
  requests:
    memory: "128Mi"  # Au lieu de 1Gi
    cpu: "50m"       # Au lieu de 500m
  limits:
    memory: "256Mi"  # Au lieu de 2Gi
    cpu: "200m"      # Au lieu de 1000m
```

**2. D√©sactivation de Kafka/Zookeeper** :
- Non critiques pour la d√©mo
- Service d'ingestion √©crit directement en base
- Simplifie l'architecture

**3. Priorisation des services critiques** :
- TimescaleDB : 256Mi max
- API Gateway : 128Mi max
- Frontend : 64Mi max

**Fichiers modifi√©s** : Tous les deployments Kubernetes

**Commit** :
```bash
git add kubernetes/*.yaml
git commit -m "perf(k8s): reduce resource limits for Oracle Free Tier (2GB RAM)"
```

---

## 8. Conflit de Certificats

### üî¥ Probl√®me

Deux objets Certificate Kubernetes se cr√©aient simultan√©ment avec le m√™me `secretName` :
- `xdr-tls-secret` (cr√©√© automatiquement par l'Ingress)
- `xdr-tls-selfsigned` (cr√©√© manuellement)

Les deux restaient en `READY=False` ind√©finiment.

### üîç Analyse

L'annotation `cert-manager.io/cluster-issuer: "letsencrypt-prod"` dans l'Ingress provoquait la cr√©ation automatique d'un Certificate par cert-manager. Ce Certificate entrait en conflit avec celui cr√©√© manuellement.

### ‚úÖ Solution

**1. Suppression de l'annotation** dans l'Ingress :
```yaml
# annotations:
#   cert-manager.io/cluster-issuer: "letsencrypt-prod"  # ‚Üê Comment√©
```

**2. Ordre de d√©ploiement strict** :
```bash
# Supprimer tous les objets Certificate et Secrets
kubectl delete certificate --all -n xdr-platform
kubectl delete secret xdr-tls-secret -n xdr-platform
kubectl delete ingress xdr-ingress-https -n xdr-platform

# Cr√©er d'abord le certificat auto-sign√©
kubectl apply -f selfsigned-cert.yaml

# Attendre qu'il soit Ready
kubectl wait --for=condition=Ready certificate/xdr-tls-selfsigned -n xdr-platform

# Puis cr√©er l'Ingress
kubectl apply -f ingress-https.yaml
```

**R√©sultat** : Un seul Certificate, passe √† `READY=True` en 10 secondes.

---

## üí° Comp√©tences D√©montr√©es

- ‚úÖ Analyse de logs : `kubectl logs`, `describe`, `get events`
- ‚úÖ Compr√©hension des erreurs : PostgreSQL, Docker, Kubernetes, NGINX
- ‚úÖ Recherche de solutions : Documentation officielle, GitHub Issues
- ‚úÖ Tests it√©ratifs : Validation apr√®s chaque modification
- ‚úÖ Pragmatisme : Choix de contournements quand n√©cessaire
- ‚úÖ Documentation : Prise de notes des erreurs et solutions
- ‚úÖ Pers√©v√©rance : R√©solution de 8+ probl√®mes majeurs
- ‚úÖ Adaptabilit√© : Ajustement de l'architecture face aux contraintes

---

## üìö Ressources Utiles

- [Kubernetes Documentation](https://kubernetes.io/docs/)
- [TimescaleDB Documentation](https://docs.timescale.com/)
- [PostgreSQL Arrays](https://www.postgresql.org/docs/current/arrays.html)
- [Oracle Cloud Security Lists](https://docs.oracle.com/en-us/iaas/Content/Network/Concepts/securitylists.htm)
- [cert-manager Documentation](https://cert-manager.io/docs/)
